{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7657582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Lab2\").getOrCreate()\n",
    "\n",
    "df=spark.read.csv(\"people.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "69341e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df1 = df.filter(col(\"age\") > 30)\n",
    "df1 = df1.withColumn(\"age_in_months\", col(\"age\") * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e50edecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 71\n",
      "+---+----------+----------+-----------+---+-------------+\n",
      "| id|first_name| last_name|     gender|age|age_in_months|\n",
      "+---+----------+----------+-----------+---+-------------+\n",
      "|  1|   Falkner|    Caddan|       Male| 72|        864.0|\n",
      "|  3|      Chev| Leavesley|       Male| 62|        744.0|\n",
      "|  4|    Lynnea|   Bardill|     Female| 77|        924.0|\n",
      "|  5|   Annabal|      Hast|     Female| 32|        384.0|\n",
      "|  6|Stephannie|    Spragg|     Female|100|       1200.0|\n",
      "|  7|    Malchy|  Cockayne| Polygender| 42|        504.0|\n",
      "|  8|   Rolland|     Doble|       Male| 35|        420.0|\n",
      "|  9| Valentijn|Gilsthorpe|       Male| 96|       1152.0|\n",
      "| 10|    Alikee| De Santos|     Female| 73|        876.0|\n",
      "| 11|     Netta|   Schanke| Non-binary| 44|        528.0|\n",
      "| 12|     Saree|   Gethins|     Female| 45|        540.0|\n",
      "| 14|      Elsa| Iwanowicz|    Agender| 55|        660.0|\n",
      "| 15| Annaliese|  Prantoni| Polygender| 98|       1176.0|\n",
      "| 16|    Georgy|     Brosh|       Male| 39|        468.0|\n",
      "| 17|  Olenolin|Ambrosetti| Polygender| 44|        528.0|\n",
      "| 18|      Davy|      Cruz|       Male| 60|        720.0|\n",
      "| 23|    Vasili|   Gulston|       Male| 50|        600.0|\n",
      "| 26|     Virge|     Ogley|       Male| 98|       1176.0|\n",
      "| 27| Augustine|    Farens|Genderqueer| 92|       1104.0|\n",
      "| 28|     Ricki|     Kienl|    Agender| 98|       1176.0|\n",
      "+---+----------+----------+-----------+---+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row_count = df1.count()\n",
    "print(f\"Number of rows: {row_count}\")\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "48adf2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg value: 51.22222222222222\n",
      "Sum: 5071.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, avg\n",
    "\n",
    "avg_val = df.select(avg(\"age\")).collect()[0][0]\n",
    "sum_val = df.select(sum(\"age\")).collect()[0][0]\n",
    "\n",
    "print(\"Avg value:\",avg_val)\n",
    "print(\"Sum:\",sum_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c7ee6386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.repartition(1)\n",
    "df1.write.csv(\"output.csv\", header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a1585e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "far  :  3\n",
      "away  :  1\n",
      "mountains  :  1\n",
      "countries  :  1\n",
      "Vokalia  :  1\n",
      "there  :  2\n",
      "live  :  2\n",
      "texts  :  2\n",
      "Separated  :  1\n",
      "in  :  2\n",
      "Bookmarksgrove  :  2\n",
      "right  :  1\n",
      "at  :  1\n",
      "coast  :  1\n",
      "of  :  10\n",
      "large  :  1\n",
      "language  :  1\n",
      "ocean  :  1\n",
      "named  :  1\n",
      "Duden  :  1\n",
      "flows  :  1\n",
      "supplies  :  1\n",
      "It  :  1\n",
      "is  :  2\n",
      "paradisematic  :  1\n",
      "country  :  1\n",
      "fly  :  1\n",
      "into  :  2\n",
      "mouth  :  1\n",
      "all-powerful  :  1\n",
      "no  :  1\n",
      "an  :  1\n",
      "One  :  1\n",
      "however  :  1\n",
      "line  :  1\n",
      "name  :  1\n",
      "Lorem  :  1\n",
      "Ipsum  :  1\n",
      "decided  :  1\n",
      "leave  :  1\n",
      "World  :  1\n",
      "The  :  1\n",
      "her  :  6\n",
      "do  :  1\n",
      "wild  :  1\n",
      "Question  :  1\n",
      "Marks  :  1\n",
      "but  :  1\n",
      "didnâ€™t  :  1\n",
      "She  :  1\n",
      "packed  :  1\n",
      "seven  :  1\n",
      "put  :  1\n",
      "initial  :  1\n",
      "belt  :  1\n",
      "way  :  1\n",
      "When  :  1\n",
      "reached  :  1\n",
      "Mountains  :  1\n",
      "last  :  1\n",
      "headline  :  1\n",
      "Alphabet  :  1\n",
      "subline  :  1\n",
      "own  :  1\n",
      "road  :  1\n",
      "Line  :  1\n",
      "Lane  :  1\n",
      "question  :  1\n",
      "ran  :  1\n",
      "Far  :  1\n",
      "behind  :  1\n",
      "the  :  19\n",
      "word  :  1\n",
      "from  :  1\n",
      "and  :  5\n",
      "Consonantia  :  1\n",
      "blind  :  3\n",
      "they  :  1\n",
      "Semantics  :  1\n",
      "a  :  5\n",
      "A  :  1\n",
      "small  :  2\n",
      "river  :  1\n",
      "by  :  2\n",
      "their  :  1\n",
      "place  :  1\n",
      "it  :  2\n",
      "with  :  1\n",
      "necessary  :  1\n",
      "regelialia  :  1\n",
      "which  :  1\n",
      "roasted  :  1\n",
      "parts  :  1\n",
      "sentences  :  1\n",
      "your  :  1\n",
      "Even  :  1\n",
      "Pointing  :  1\n",
      "has  :  1\n",
      "control  :  1\n",
      "about  :  1\n",
      "almost  :  1\n",
      "unorthographic  :  1\n",
      "life  :  1\n",
      "day  :  1\n",
      "text  :  1\n",
      "to  :  2\n",
      "for  :  1\n",
      "Grammar  :  1\n",
      "Big  :  1\n",
      "Oxmox  :  1\n",
      "advised  :  1\n",
      "not  :  1\n",
      "so  :  1\n",
      "because  :  1\n",
      "were  :  1\n",
      "thousands  :  1\n",
      "bad  :  1\n",
      "Commas  :  1\n",
      "devious  :  1\n",
      "Semikoli  :  1\n",
      "Little  :  1\n",
      "Blind  :  1\n",
      "Text  :  1\n",
      "listen  :  1\n",
      "versalia  :  1\n",
      "made  :  1\n",
      "herself  :  1\n",
      "on  :  2\n",
      "she  :  2\n",
      "first  :  1\n",
      "hills  :  1\n",
      "Italic  :  1\n",
      "had  :  1\n",
      "view  :  1\n",
      "back  :  1\n",
      "skyline  :  1\n",
      "hometown  :  1\n",
      "Village  :  1\n",
      "Pityful  :  1\n",
      "rethoric  :  1\n",
      "over  :  1\n",
      "cheek  :  1\n",
      "then  :  1\n"
     ]
    }
   ],
   "source": [
    "def clean(txt):\n",
    "    txt=txt.replace('.','')\n",
    "    txt=txt.replace(',','')\n",
    "    return txt\n",
    "\n",
    "text = spark.sparkContext.textFile(\"demo.txt\")\n",
    "words = text.flatMap(lambda line: clean(line).split())\n",
    "words = words.map(lambda word: (word, 1)) \n",
    "words = words.reduceByKey(lambda a, b: a + b) \n",
    "for i in words.collect():\n",
    "    print(i[0],\" : \",i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "06c36fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad78835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
